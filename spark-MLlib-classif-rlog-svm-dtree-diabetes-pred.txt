
val df = spark.read.format("csv").option("header","true").load("diabetes_pred/diabetes_data_upload.csv")

df.printSchema
root
 |-- Age: string (nullable = true)
 |-- Gender: string (nullable = true)
 |-- Polyuria: string (nullable = true)
 |-- Polydipsia: string (nullable = true)
 |-- sudden weight loss: string (nullable = true)
 |-- weakness: string (nullable = true)
 |-- Polyphagia: string (nullable = true)
 |-- Genital thrush: string (nullable = true)
 |-- visual blurring: string (nullable = true)
 |-- Itching: string (nullable = true)
 |-- Irritability: string (nullable = true)
 |-- delayed healing: string (nullable = true)
 |-- partial paresis: string (nullable = true)
 |-- muscle stiffness: string (nullable = true)
 |-- Alopecia: string (nullable = true)
 |-- Obesity: string (nullable = true)
 |-- class: string (nullable = true)
 
val rdd = df.rdd.map( x => x.toSeq.toArray)

val rdd1 = rdd.map( x => x.map( y => y.toString ))

rdd1.first
res2: Array[String] = Array(40, Male, No, Yes, No, Yes, No, No, No, Yes, No, Yes, No, Yes, Yes, Yes, Positive)

val categ_zerone = Map( "No" -> 0, "Yes" -> 1, "Female" -> 0, "Male" -> 1, "Negative" -> 0, "Positive" -> 1)

val rdd2 = rdd1.map( y => y.map( x => {
                     try { x.toDouble } catch { case _ : Throwable => categ_zerone(x).toString.toDouble } }))

rdd2.first
res3: Array[Double] = Array(40.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0)

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

val data = rdd2.map(x => {
   val arr_size = x.size - 1
   val l = x(arr_size)
   val f = x.slice(0, arr_size)
   LabeledPoint(l, Vectors.dense(f))
 })
 
val sets = data.randomSplit(Array(0.7,0.3))
val trainSet = sets(0)
val testSet = sets(1)

trainSet.cache

---- MLlib logistic regression --------------

import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
val numIterations = 100
val model = LogisticRegressionWithSGD.train(trainSet, numIterations)

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(20)
res6: Array[(Double, Double)] = Array((0.0,1.0), (0.0,1.0), (0.0,1.0), (0.0,1.0), (0.0,1.0), (0.0,1.0), (0.0,1.0), (0.0,1.0), (0.0,1.0), (0.0,1.0), (0.0,1.0), (0.0,1.0), (0.0,1.0), (0.0,1.0), (0.0,1.0), (0.0,1.0), (0.0,1.0), (1.0,1.0), (0.0,1.0), (0.0,1.0))

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 78
validPredicts.count                            // 156
model.getClass.getSimpleName
metrics.areaUnderPR   // 0.8351063829787234
metrics.areaUnderROC  // 0.5851063829787234

---- MLlib SVM regression --------------

import org.apache.spark.mllib.classification.SVMWithSGD
val numIterations = 100
val model = SVMWithSGD.train(trainSet, numIterations)

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(20)
res12: Array[(Double, Double)] = Array((1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0))

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 94
validPredicts.count                            // 156
model.getClass.getSimpleName
metrics.areaUnderPR   // 0.6025641025641025
metrics.areaUnderROC  // 0.5

---- MLlib Maive Bayes regression --------------

import org.apache.spark.mllib.classification.NaiveBayes
val model = NaiveBayes.train(trainSet)

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(20)
res18: Array[(Double, Double)] = Array((1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (0.0,1.0), (0.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (0.0,1.0))

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 138
validPredicts.count                            // 156
model.getClass.getSimpleName
metrics.areaUnderPR   //  0.8833537915984725
metrics.areaUnderROC  //  0.8768016472203157

-------------------------

using naive-bayes model

val vect1 = Vectors.dense(51,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0)

model.predict(vect1)
res24: Double = 0.0
